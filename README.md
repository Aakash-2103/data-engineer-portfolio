ğŸš€ğŸ“Š Near Real-Time Stock Data Pipeline â€” API â†’ AWS S3 â†’ Snowflake








In todayâ€™s trading world, â³ speed = money.
This project demonstrates how to design and implement an end-to-end stock market data pipeline that fetches daily stock prices from the MarketStack API, processes and stores them in AWS S3, and makes them analytics-ready in Snowflake for fast BI visualizations.

ğŸ›  Tech Stack

Extraction â†’ MarketStack REST API (GET request, Access Key Auth) tested via Postman, automated with Python (requests, pandas)

Transformation â†’ Python (pandas) for cleaning, deduplication, timestamp formatting

Storage â†’ AWS S3 (secure landing zone, integrated with Snowflake via IAM Role & ARN)

Analytics â†’ Snowflake (external stage from S3, optimized for BI queries)

âš™ Workflow
1ï¸âƒ£ API Data Extraction

Tested API endpoints using Postman (GET request with access key auth)

Endpoint: http://api.marketstack.com/v1/eod

Automated with extract_market.py (reads API key from config.json)

2ï¸âƒ£ Data Transformation

Script: transform.py

Steps:

Removed duplicates

Filled missing values

Converted timestamps to analytics-friendly formats

Exported as transformed_stock_data_<timestamp>.csv

3ï¸âƒ£ AWS S3 Storage

Configured AWS credentials via config.json

Script: s3.py â†’ uploads transformed CSV to S3 bucket

IAM Role created with required policies & ARN

Bucket policies attached to enable Snowflake access

4ï¸âƒ£ Snowflake Integration

Created Snowflake Storage Integration linked to AWS IAM Role

Defined External Stage pointing to S3 bucket

Loaded data into MARKET_STACK table using snowflake_etl.sql

Optimized for fast BI queries, reducing query times for analysts

ğŸ“‚ Repository Structure
â”œâ”€â”€ config.json                      # API keys & credentials (ignored in repo)
â”œâ”€â”€ extract_market.py                # Fetch stock data from MarketStack API
â”œâ”€â”€ transform.py                     # Clean & transform raw data
â”œâ”€â”€ s3.py                            # Upload transformed data to AWS S3
â”œâ”€â”€ snowflake_etl.sql                # SQL script for Snowflake stage + load
â”œâ”€â”€ raw_stock_data_<timestamp>.csv   # Example raw dataset
â”œâ”€â”€ transformed_stock_data_<ts>.csv  # Example transformed dataset
â”œâ”€â”€ README.md                        # Project documentation
â””â”€â”€ .DS_Store                        # (ignore, MacOS artifact)

ğŸ’¡ Why This Architecture?

â˜ AWS S3 â†’ Low-cost, scalable storage, easy integration with Snowflake

âš¡ Snowflake External Stage â†’ Directly queries S3 data, reducing ingestion overhead

ğŸ“Š BI-Ready Data â†’ Analysts can access clean, transformed stock data with minimal latency

ğŸ“ˆ Business Impact

âš¡ Faster Decisions â†’ Traders access fresh daily stock data within minutes

ğŸ“Š Scalability â†’ Pipeline handles large historical + real-time datasets

ğŸ’° Cost-Efficient â†’ Pay-as-you-go storage + Snowflake compute only when needed

ğŸ“‰ Reduced Query Time â†’ Snowflake staging + optimized schema cuts BI query time significantly

ğŸš€ How to Run Locally

Clone this repo:

git clone https://github.com/Aakash-2103/data-engineer-portfolio.git
cd data-engineer-portfolio


Install dependencies:

pip install -r requirements.txt


Create config.json (not in repo) with your credentials:

{
  "marketstack_api_key": "YOUR_API_KEY",
  "aws_access_key": "YOUR_AWS_ACCESS_KEY",
  "aws_secret_key": "YOUR_AWS_SECRET_KEY",
  "snowflake_user": "YOUR_USER",
  "snowflake_password": "YOUR_PASSWORD",
  "snowflake_account": "YOUR_ACCOUNT"
}


Run pipeline step by step:

python extract_market.py
python transform.py
python s3.py


Run Snowflake script:

-- snowflake_etl.sql
CREATE STAGE marketstack_stage URL='s3://<bucket-name>' STORAGE_INTEGRATION = my_integration;
COPY INTO MARKET_STACK FROM @marketstack_stage FILE_FORMAT = (TYPE = CSV);

ğŸ“Œ Future Improvements

 Automate with Airflow or AWS Lambda

 Add real-time streaming with Kafka/Kinesis

 Implement CI/CD pipeline for cloud deployment

âœ¨ Author

ğŸ‘¤ Aakash Sandela
ğŸ”— LinkedIn â€¢ GitHub
